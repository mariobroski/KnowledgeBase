#!/usr/bin/env python3
"""
Pe≈Çny pipeline ewaluacji RAGBench + FinanceBench
Uruchamia import, ewaluacjƒô i generowanie raport√≥w
"""

import os
import sys
import json
import logging
import subprocess
import time
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
from advanced_metrics import AdvancedEvaluator
from literature_comparison import LiteratureComparator
from trend_analysis import TrendAnalyzer

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class FullPipelineRunner:
    """Runner pe≈Çnego pipeline'u ewaluacji"""
    
    def __init__(self):
        self.eval_dir = Path(__file__).parent
        self.results_dir = self.eval_dir / "results"
        self.reports_dir = self.eval_dir / "reports"
        
        # Konfiguracja domen
        self.ragbench_domains = ["FinQA", "TAT-QA", "TechQA", "CUAD", "EManual"]
        self.financebench_domains = ["FinanceBench", "FinancialQA", "FinNLP"]
        
        # Konfiguracja polityk
        self.policies = ["text", "facts", "graph", "hybrid"]
        
        # Konfiguracja API
        self.api_base_url = "http://localhost:8000"
        
        # Nowe komponenty
        self.advanced_evaluator = AdvancedEvaluator()
        self.literature_comparator = LiteratureComparator()
        self.trend_analyzer = TrendAnalyzer()
        
    def run_full_pipeline(self, domains: List[str] = None, policies: List[str] = None):
        """
        Uruchamia pe≈Çny pipeline ewaluacji
        
        Args:
            domains: Lista domen do testowania
            policies: Lista polityk do testowania
        """
        if domains is None:
            domains = self.ragbench_domains + self.financebench_domains
        
        if policies is None:
            policies = self.policies
        
        logger.info(f"Rozpoczynanie pe≈Çnego pipeline'u ewaluacji")
        logger.info(f"Domeny: {domains}")
        logger.info(f"Polityki: {policies}")
        
        start_time = datetime.now()
        
        try:
            # Krok 1: Sprawd≈∫ dostƒôpno≈õƒá API
            if not self._check_api_availability():
                logger.error("API nie jest dostƒôpne. Uruchom backend przed kontynuowaniem.")
                return False
            
            # Krok 2: Import korpus√≥w
            import_results = self._import_corpus(domains)
            if not import_results:
                logger.error("B≈ÇƒÖd importu korpus√≥w")
                return False
            
            # Krok 3: Ewaluacja
            eval_results = self._run_evaluation(domains, policies)
            if not eval_results:
                logger.error("B≈ÇƒÖd ewaluacji")
                return False
            
            # Krok 4: Generowanie raport√≥w
            report_results = self._generate_reports(domains)
            if not report_results:
                logger.error("B≈ÇƒÖd generowania raport√≥w")
                return False
            
            # Krok 5: Zaawansowana analiza
            advanced_results = self._run_advanced_analysis(domains, policies)
            if not advanced_results:
                logger.warning("B≈ÇƒÖd zaawansowanej analizy")
            
            # Krok 6: Por√≥wnanie z literaturƒÖ
            literature_results = self._run_literature_comparison(domains, policies)
            if not literature_results:
                logger.warning("B≈ÇƒÖd por√≥wnania z literaturƒÖ")
            
            # Krok 7: Analiza trend√≥w
            trend_results = self._run_trend_analysis(domains, policies)
            if not trend_results:
                logger.warning("B≈ÇƒÖd analizy trend√≥w")
            
            # Krok 8: Podsumowanie
            self._generate_summary(domains, policies, start_time)
            
            logger.info("‚úÖ Pe≈Çny pipeline ewaluacji zako≈Ñczony pomy≈õlnie")
            return True
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd w pe≈Çnym pipeline'u: {e}")
            return False
    
    def _check_api_availability(self) -> bool:
        """Sprawdza dostƒôpno≈õƒá API"""
        logger.info("Sprawdzanie dostƒôpno≈õci API...")
        
        try:
            import requests
            response = requests.get(f"{self.api_base_url}/health", timeout=10)
            if response.status_code == 200:
                logger.info("‚úÖ API jest dostƒôpne")
                return True
            else:
                logger.error(f"‚ùå API zwr√≥ci≈Ço status {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"‚ùå B≈ÇƒÖd po≈ÇƒÖczenia z API: {e}")
            return False
    
    def _import_corpus(self, domains: List[str]) -> bool:
        """Importuje korpusy"""
        logger.info("Rozpoczynanie importu korpus√≥w...")
        
        success_count = 0
        
        for domain in domains:
            logger.info(f"Import korpusu {domain}...")
            
            try:
                if domain in self.ragbench_domains:
                    # Import RAGBench
                    result = subprocess.run([
                        sys.executable, 
                        str(self.eval_dir / "ingest_ragbench.py"),
                        domain
                    ], capture_output=True, text=True, timeout=300)
                elif domain in self.financebench_domains:
                    # Import FinanceBench
                    result = subprocess.run([
                        sys.executable,
                        str(self.eval_dir / "ingest_financebench.py"),
                        domain
                    ], capture_output=True, text=True, timeout=300)
                else:
                    logger.warning(f"Nieznana domena: {domain}")
                    continue
                
                if result.returncode == 0:
                    logger.info(f"‚úÖ Import korpusu {domain} zako≈Ñczony pomy≈õlnie")
                    success_count += 1
                else:
                    logger.error(f"‚ùå B≈ÇƒÖd importu korpusu {domain}: {result.stderr}")
                
            except subprocess.TimeoutExpired:
                logger.error(f"‚è∞ Timeout importu korpusu {domain}")
            except Exception as e:
                logger.error(f"üí• B≈ÇƒÖd importu korpusu {domain}: {e}")
        
        logger.info(f"Import zako≈Ñczony: {success_count}/{len(domains)} korpus√≥w")
        return success_count > 0
    
    def _run_evaluation(self, domains: List[str], policies: List[str]) -> bool:
        """Uruchamia ewaluacjƒô"""
        logger.info("Rozpoczynanie ewaluacji...")
        
        success_count = 0
        
        for domain in domains:
            logger.info(f"Ewaluacja domeny {domain}...")
            
            try:
                # Uruchom ewaluacjƒô dla domeny
                result = subprocess.run([
                    sys.executable,
                    str(self.eval_dir / "run_eval.py"),
                    domain,
                    ",".join(policies)
                ], capture_output=True, text=True, timeout=1800)  # 30 minut timeout
                
                if result.returncode == 0:
                    logger.info(f"‚úÖ Ewaluacja domeny {domain} zako≈Ñczona pomy≈õlnie")
                    success_count += 1
                else:
                    logger.error(f"‚ùå B≈ÇƒÖd ewaluacji domeny {domain}: {result.stderr}")
                
            except subprocess.TimeoutExpired:
                logger.error(f"‚è∞ Timeout ewaluacji domeny {domain}")
            except Exception as e:
                logger.error(f"üí• B≈ÇƒÖd ewaluacji domeny {domain}: {e}")
        
        logger.info(f"Ewaluacja zako≈Ñczona: {success_count}/{len(domains)} domen")
        return success_count > 0
    
    def _generate_reports(self, domains: List[str]) -> bool:
        """Generuje raporty"""
        logger.info("Rozpoczynanie generowania raport√≥w...")
        
        try:
            # Generuj raporty dla ka≈ºdej domeny
            for domain in domains:
                logger.info(f"Generowanie raportu dla domeny {domain}...")
                
                result = subprocess.run([
                    sys.executable,
                    str(self.eval_dir / "generate_report.py"),
                    domain
                ], capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    logger.info(f"‚úÖ Raport dla domeny {domain} wygenerowany")
                else:
                    logger.error(f"‚ùå B≈ÇƒÖd generowania raportu dla domeny {domain}: {result.stderr}")
            
            # Generuj zbiorczy raport
            logger.info("Generowanie zbiorczego raportu...")
            
            result = subprocess.run([
                sys.executable,
                str(self.eval_dir / "generate_report.py"),
                ",".join(domains),
                "--combined"
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                logger.info("‚úÖ Zbiorczy raport wygenerowany")
                return True
            else:
                logger.error(f"‚ùå B≈ÇƒÖd generowania zbiorczego raportu: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"üí• B≈ÇƒÖd generowania raport√≥w: {e}")
            return False
    
    def _generate_summary(self, domains: List[str], policies: List[str], start_time: datetime):
        """Generuje podsumowanie"""
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("üìä PODSUMOWANIE PE≈ÅNEGO PIPELINE'U EWALUACJI")
        logger.info("=" * 60)
        logger.info(f"‚è∞ Czas trwania: {duration}")
        logger.info(f"üìã Domeny: {', '.join(domains)}")
        logger.info(f"üîß Polityki: {', '.join(policies)}")
        logger.info(f"üìÅ Wyniki: {self.results_dir}")
        logger.info(f"üìä Raporty: {self.reports_dir}")
        
        # Sprawd≈∫ pliki wynik√≥w
        result_files = list(self.results_dir.glob("*.csv"))
        report_files = list(self.reports_dir.glob("*.json"))
        
        logger.info(f"üìÑ Pliki wynik√≥w: {len(result_files)}")
        logger.info(f"üìä Pliki raport√≥w: {len(report_files)}")
        
        # Wy≈õwietl najwa≈ºniejsze pliki
        if result_files:
            logger.info("üìÑ Najwa≈ºniejsze pliki wynik√≥w:")
            for file in result_files[:5]:  # Poka≈º pierwsze 5
                logger.info(f"  - {file.name}")
        
        if report_files:
            logger.info("üìä Najwa≈ºniejsze pliki raport√≥w:")
            for file in report_files[:5]:  # Poka≈º pierwsze 5
                logger.info(f"  - {file.name}")
        
        logger.info("=" * 60)
    
    def _run_advanced_analysis(self, domains: List[str], policies: List[str]) -> bool:
        """Uruchamia zaawansowanƒÖ analizƒô metryk"""
        logger.info("Rozpoczynanie zaawansowanej analizy...")
        
        try:
            # Za≈Çaduj wyniki ewaluacji
            for domain in domains:
                csv_files = list(self.results_dir.glob(f"{domain}_results_*.csv"))
                
                if not csv_files:
                    logger.warning(f"Brak wynik√≥w dla domeny {domain}")
                    continue
                
                # Za≈Çaduj najnowszy plik
                latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
                df = pd.read_csv(latest_file)
                
                # Analizuj zaawansowane metryki
                for _, row in df.iterrows():
                    advanced_metrics = self.advanced_evaluator.calculate_advanced_metrics(
                        query=row['query'],
                        response=row['response'],
                        context=json.loads(row.get('context', '{}')),
                        ground_truth=json.loads(row.get('ground_truth', '[]'))
                    )
                    
                    # Generuj raport jako≈õci
                    quality_report = self.advanced_evaluator.generate_quality_report(advanced_metrics)
                    
                    # Zapisz wyniki
                    advanced_file = self.results_dir / f"{domain}_advanced_metrics.json"
                    with open(advanced_file, 'w', encoding='utf-8') as f:
                        json.dump(quality_report, f, indent=2, ensure_ascii=False)
            
            logger.info("‚úÖ Zaawansowana analiza zako≈Ñczona")
            return True
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd zaawansowanej analizy: {e}")
            return False
    
    def _run_literature_comparison(self, domains: List[str], policies: List[str]) -> bool:
        """Uruchamia por√≥wnanie z literaturƒÖ"""
        logger.info("Rozpoczynanie por√≥wnania z literaturƒÖ...")
        
        try:
            comparison_results = []
            
            for domain in domains:
                # Za≈Çaduj wyniki
                csv_files = list(self.results_dir.glob(f"{domain}_results_*.csv"))
                
                if not csv_files:
                    continue
                
                latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)
                df = pd.read_csv(latest_file)
                
                # Przygotuj wyniki dla por√≥wnania
                our_results = {}
                for policy in policies:
                    policy_data = df[df['policy'] == policy]
                    if not policy_data.empty:
                        our_results[policy] = {
                            "relevance": policy_data['relevance'].mean(),
                            "utilization": policy_data['utilization'].mean(),
                            "adherence": policy_data['adherence'].mean(),
                            "completeness": policy_data['completeness'].mean()
                        }
                
                # Por√≥wnaj z literaturƒÖ
                comparison_result = self.literature_comparator.compare_with_literature(
                    our_results=our_results,
                    benchmark="RAGBench" if domain in self.ragbench_domains else "FinanceBench",
                    domain=domain
                )
                
                comparison_results.append(comparison_result)
            
            # Generuj raport por√≥wnawczy
            if comparison_results:
                report = self.literature_comparator.generate_comparison_report(comparison_results)
                self.literature_comparator.save_comparison_report(
                    report, 
                    str(self.reports_dir / "literature_comparison_report.json")
                )
            
            logger.info("‚úÖ Por√≥wnanie z literaturƒÖ zako≈Ñczone")
            return True
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd por√≥wnania z literaturƒÖ: {e}")
            return False
    
    def _run_trend_analysis(self, domains: List[str], policies: List[str]) -> bool:
        """Uruchamia analizƒô trend√≥w"""
        logger.info("Rozpoczynanie analizy trend√≥w...")
        
        try:
            # Za≈Çaduj dane historyczne
            for domain in domains:
                csv_files = list(self.results_dir.glob(f"{domain}_results_*.csv"))
                
                for csv_file in csv_files:
                    df = pd.read_csv(csv_file)
                    
                    # Dodaj punkty danych do analizatora
                    for _, row in df.iterrows():
                        timestamp = datetime.now()  # W rzeczywisto≈õci z pliku
                        
                        # Dodaj r√≥≈ºne metryki
                        for metric in ['relevance', 'utilization', 'adherence', 'completeness']:
                            if metric in row:
                                self.trend_analyzer.add_data_point(
                                    timestamp=timestamp,
                                    metric=metric,
                                    value=row[metric],
                                    policy=row['policy'],
                                    domain=domain
                                )
            
            # Wykonaj analizƒô trend√≥w
            trend_report = self.trend_analyzer.generate_trend_report()
            
            # Zapisz raport
            with open(self.reports_dir / "trend_analysis_report.json", 'w', encoding='utf-8') as f:
                json.dump(trend_report, f, indent=2, ensure_ascii=False)
            
            # Stw√≥rz wykresy
            for metric in ['relevance', 'utilization', 'adherence', 'completeness']:
                self.trend_analyzer.plot_trends(
                    metric=metric,
                    save_path=str(self.reports_dir / f"trend_{metric}.png")
                )
            
            logger.info("‚úÖ Analiza trend√≥w zako≈Ñczona")
            return True
            
        except Exception as e:
            logger.error(f"B≈ÇƒÖd analizy trend√≥w: {e}")
            return False
    
    def run_quick_test(self, domain: str = "FinQA", policy: str = "text"):
        """
        Uruchamia szybki test dla jednej domeny i polityki
        
        Args:
            domain: Domena do testowania
            policy: Polityka do testowania
        """
        logger.info(f"Rozpoczynanie szybkiego testu: {domain} + {policy}")
        
        try:
            # Sprawd≈∫ API
            if not self._check_api_availability():
                return False
            
            # Import korpusu
            logger.info(f"Import korpusu {domain}...")
            if domain in self.ragbench_domains:
                result = subprocess.run([
                    sys.executable,
                    str(self.eval_dir / "ingest_ragbench.py"),
                    domain
                ], capture_output=True, text=True, timeout=300)
            elif domain in self.financebench_domains:
                result = subprocess.run([
                    sys.executable,
                    str(self.eval_dir / "ingest_financebench.py"),
                    domain
                ], capture_output=True, text=True, timeout=300)
            else:
                logger.error(f"Nieznana domena: {domain}")
                return False
            
            if result.returncode != 0:
                logger.error(f"B≈ÇƒÖd importu: {result.stderr}")
                return False
            
            # Ewaluacja
            logger.info(f"Ewaluacja {domain} + {policy}...")
            result = subprocess.run([
                sys.executable,
                str(self.eval_dir / "run_eval.py"),
                domain,
                policy
            ], capture_output=True, text=True, timeout=600)
            
            if result.returncode != 0:
                logger.error(f"B≈ÇƒÖd ewaluacji: {result.stderr}")
                return False
            
            # Raport
            logger.info(f"Generowanie raportu...")
            result = subprocess.run([
                sys.executable,
                str(self.eval_dir / "generate_report.py"),
                domain
            ], capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                logger.error(f"B≈ÇƒÖd generowania raportu: {result.stderr}")
                return False
            
            logger.info("‚úÖ Szybki test zako≈Ñczony pomy≈õlnie")
            return True
            
        except Exception as e:
            logger.error(f"üí• B≈ÇƒÖd w szybkim te≈õcie: {e}")
            return False


def main():
    """G≈Ç√≥wna funkcja"""
    if len(sys.argv) < 2:
        print("U≈ºycie:")
        print("  python run_full_pipeline.py --full [domains] [policies]")
        print("  python run_full_pipeline.py --quick [domain] [policy]")
        print("  python run_full_pipeline.py --domains")
        print("")
        print("Przyk≈Çady:")
        print("  python run_full_pipeline.py --full")
        print("  python run_full_pipeline.py --full FinQA,TAT-QA text,facts")
        print("  python run_full_pipeline.py --quick FinQA text")
        print("  python run_full_pipeline.py --domains")
        sys.exit(1)
    
    runner = FullPipelineRunner()
    
    if sys.argv[1] == "--full":
        # Pe≈Çny pipeline
        domains = sys.argv[2].split(",") if len(sys.argv) > 2 else None
        policies = sys.argv[3].split(",") if len(sys.argv) > 3 else None
        
        success = runner.run_full_pipeline(domains, policies)
        sys.exit(0 if success else 1)
        
    elif sys.argv[1] == "--quick":
        # Szybki test
        domain = sys.argv[2] if len(sys.argv) > 2 else "FinQA"
        policy = sys.argv[3] if len(sys.argv) > 3 else "text"
        
        success = runner.run_quick_test(domain, policy)
        sys.exit(0 if success else 1)
        
    elif sys.argv[1] == "--domains":
        # Wy≈õwietl dostƒôpne domeny
        print("Dostƒôpne domeny RAGBench:")
        for domain in runner.ragbench_domains:
            print(f"  - {domain}")
        
        print("\nDostƒôpne domeny FinanceBench:")
        for domain in runner.financebench_domains:
            print(f"  - {domain}")
        
        print(f"\nDostƒôpne polityki: {', '.join(runner.policies)}")
        
    else:
        print("Nieznana opcja. U≈ºyj --help dla pomocy.")
        sys.exit(1)


if __name__ == "__main__":
    main()
